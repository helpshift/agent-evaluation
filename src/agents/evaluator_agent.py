"""
Evaluator Agent - LLM-as-Judge for StoryFlowAgent evaluation.

This agent evaluates stories generated by StoryFlowAgent using
the ADK pattern of agent-evaluating-agent.
"""

from typing import Optional
from pydantic import BaseModel, Field

from google.adk.agents import LlmAgent


# --- Evaluation Schemas ---
class EvaluationInput(BaseModel):
    """Input schema for the evaluator."""
    test_id: str = Field(description="The test case ID")
    topic: str = Field(description="The original story topic")
    generated_story: str = Field(description="The story generated by the agent")
    expected_behavior: str = Field(description="Expected behavior description")
    evaluation_criteria: list[str] = Field(description="Criteria to evaluate against")


class EvaluationResult(BaseModel):
    """Output schema for the evaluator."""
    test_id: str = Field(description="The test case ID")
    overall_score: float = Field(description="Overall score from 0.0 to 1.0")
    verdict: str = Field(description="PASS, PARTIAL, or FAIL")
    story_coherence: float = Field(description="Story coherence score 0.0-1.0")
    topic_adherence: float = Field(description="How well story matches topic 0.0-1.0")
    creativity: float = Field(description="Creativity and engagement score 0.0-1.0")
    grammar_quality: float = Field(description="Grammar and language quality 0.0-1.0")
    reasoning: str = Field(description="Detailed reasoning for the evaluation")
    suggestions: Optional[str] = Field(default=None, description="Improvement suggestions")


# --- Evaluator Agent Definition ---
EVALUATOR_INSTRUCTIONS = """You are an Expert Story Evaluator AI.

Your task is to evaluate stories generated by a creative writing agent.

## Evaluation Process

1. **Read the Story**: Carefully read the generated story
2. **Compare to Topic**: Check if the story addresses the given topic
3. **Assess Quality**: Evaluate based on the criteria provided
4. **Score Fairly**: Provide scores between 0.0 and 1.0

## Scoring Guidelines

### Story Coherence (story_coherence)
- 0.9-1.0: Perfect flow, logical progression, no inconsistencies
- 0.7-0.8: Good flow with minor issues
- 0.5-0.6: Some confusing elements but understandable
- 0.3-0.4: Significant coherence issues
- 0.0-0.2: Incoherent or nonsensical

### Topic Adherence (topic_adherence)
- 0.9-1.0: Story directly addresses and explores the topic
- 0.7-0.8: Mostly on topic with minor deviations
- 0.5-0.6: Partially addresses topic
- 0.3-0.4: Loosely related to topic
- 0.0-0.2: Completely off-topic

### Creativity (creativity)
- 0.9-1.0: Highly original, unexpected elements, engaging
- 0.7-0.8: Creative with interesting elements
- 0.5-0.6: Standard storytelling, some creativity
- 0.3-0.4: Generic or predictable
- 0.0-0.2: No creative elements, extremely basic

### Grammar Quality (grammar_quality)
- 0.9-1.0: Flawless grammar and style
- 0.7-0.8: Minor grammar issues
- 0.5-0.6: Some noticeable errors
- 0.3-0.4: Frequent errors affecting readability
- 0.0-0.2: Major grammar problems

## Verdict Rules
- **PASS**: overall_score >= 0.7
- **PARTIAL**: overall_score >= 0.5 and < 0.7
- **FAIL**: overall_score < 0.5

Calculate overall_score as weighted average:
overall_score = (story_coherence * 0.3) + (topic_adherence * 0.3) + (creativity * 0.25) + (grammar_quality * 0.15)

Respond ONLY with a JSON object matching the EvaluationResult schema.
"""


def create_evaluator_agent(model: str = "gemini-2.5-pro") -> LlmAgent:
    """Create an evaluator agent for story assessment."""
    return LlmAgent(
        name="StoryEvaluator",
        model=model,
        instruction=EVALUATOR_INSTRUCTIONS,
        description="Evaluates stories generated by the StoryFlowAgent",
        input_schema=EvaluationInput,
        output_schema=EvaluationResult,
    )


# Module-level agent for adk web
evaluator_agent = create_evaluator_agent()
