"""
Comprehensive evaluation test suite for StoryFlowAgent.

Uses the evaluator agent (LLM-as-judge) to evaluate stories
generated by StoryFlowAgent against the golden dataset.
"""

import asyncio
import json
import os
from pathlib import Path
from datetime import datetime
from typing import Any

import pytest
from dotenv import load_dotenv

from google.adk.agents import LlmAgent
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types

# Load environment variables
load_dotenv()

# Import agents
from src.agents.story_flow_agent import StoryFlowAgent, root_agent as story_agent
from src.agents.evaluator_agent import create_evaluator_agent, EvaluationInput


# --- Constants ---
APP_NAME = "story_evaluation"
USER_ID = "eval_user"


def load_golden_dataset() -> list[dict]:
    """Load the golden dataset from JSON file."""
    dataset_path = Path(__file__).parent / "data" / "story_eval_dataset.json"
    with open(dataset_path, "r") as f:
        return json.load(f)


async def run_story_agent(
    runner: Runner,
    session_id: str,
    topic: str,
    query: str
) -> str:
    """Run the StoryFlowAgent and collect the final story from session state."""
    
    # Create message content
    content = types.Content(
        role="user",
        parts=[types.Part(text=query)]
    )
    
    # Run the agent - consume all events
    async for event in runner.run_async(
        user_id=USER_ID,
        session_id=session_id,
        new_message=content
    ):
        pass  # Just run through the workflow
    
    # Get the story from session state after workflow completes
    session = await runner.session_service.get_session(
        app_name=runner.app_name,
        user_id=USER_ID,
        session_id=session_id
    )
    
    return session.state.get("current_story", "")


async def run_evaluator(
    runner: Runner,
    session_id: str,
    eval_input: EvaluationInput
) -> dict[str, Any]:
    """Run the evaluator agent and get the evaluation result."""
    
    # Convert input to JSON
    input_json = eval_input.model_dump_json()
    
    content = types.Content(
        role="user",
        parts=[types.Part(text=input_json)]
    )
    
    # Run evaluation
    final_response = ""
    async for event in runner.run_async(
        user_id=USER_ID,
        session_id=session_id,
        new_message=content
    ):
        if event.is_final_response() and event.content and event.content.parts:
            final_response = event.content.parts[0].text
    
    # Parse JSON response
    try:
        # Try to extract JSON from response
        if "```json" in final_response:
            json_str = final_response.split("```json")[1].split("```")[0]
        elif "```" in final_response:
            json_str = final_response.split("```")[1].split("```")[0]
        else:
            json_str = final_response
        
        return json.loads(json_str.strip())
    except (json.JSONDecodeError, IndexError) as e:
        return {
            "test_id": eval_input.test_id,
            "overall_score": 0.0,
            "verdict": "ERROR",
            "reasoning": f"Failed to parse evaluator response: {str(e)}",
            "raw_response": final_response
        }


class TestStoryFlowEvaluation:
    """Test class for StoryFlowAgent evaluation."""
    
    @pytest.fixture
    def golden_dataset(self) -> list[dict]:
        """Load the golden dataset."""
        return load_golden_dataset()
    
    @pytest.fixture
    def story_runner(self) -> Runner:
        """Create a runner for the story agent."""
        session_service = InMemorySessionService()
        return Runner(
            agent=story_agent,
            app_name=APP_NAME,
            session_service=session_service
        )
    
    @pytest.fixture
    def evaluator_runner(self) -> Runner:
        """Create a runner for the evaluator agent."""
        evaluator = create_evaluator_agent()
        session_service = InMemorySessionService()
        return Runner(
            agent=evaluator,
            app_name=f"{APP_NAME}_evaluator",
            session_service=session_service
        )
    
    @pytest.mark.asyncio
    async def test_single_story_generation(self, story_runner):
        """Test a single story generation."""
        session_service = story_runner.session_service
        session_id = "test_single_001"
        
        # Create session with initial state
        await session_service.create_session(
            app_name=APP_NAME,
            user_id=USER_ID,
            session_id=session_id,
            state={"topic": "a brave knight saving a village"}
        )
        
        story = await run_story_agent(
            story_runner,
            session_id,
            "a brave knight saving a village",
            "Write a story about a brave knight"
        )
        
        assert story, "Story should not be empty"
        assert len(story) > 50, "Story should have substantial content"
        print(f"\nGenerated Story:\n{story}")
    
    @pytest.mark.asyncio
    async def test_evaluate_single_story(self, evaluator_runner):
        """Test the evaluator on a sample story."""
        session_service = evaluator_runner.session_service
        session_id = "eval_single_001"
        
        await session_service.create_session(
            app_name=f"{APP_NAME}_evaluator",
            user_id=USER_ID,
            session_id=session_id
        )
        
        # Sample story for evaluation
        eval_input = EvaluationInput(
            test_id="test_001",
            topic="a brave astronaut exploring Mars",
            generated_story="Captain Sarah gazed at the rusty Martian landscape through her helmet visor. After three months of solitary exploration, she had finally found itâ€”an ancient cave system hidden beneath the red dunes. Her heart raced as she stepped inside, knowing that whatever secrets lay within would change humanity's understanding of the universe forever.",
            expected_behavior="Generates a coherent story about space exploration",
            evaluation_criteria=["story_coherence", "topic_adherence", "creativity"]
        )
        
        result = await run_evaluator(evaluator_runner, session_id, eval_input)
        
        assert "overall_score" in result, "Result should have overall_score"
        assert "verdict" in result, "Result should have verdict"
        print(f"\nEvaluation Result:\n{json.dumps(result, indent=2)}")
    
    @pytest.mark.asyncio
    async def test_full_evaluation_pipeline(self, golden_dataset, story_runner, evaluator_runner):
        """Run full evaluation on a subset of the golden dataset."""
        
        # Use first 5 test cases for quick testing
        test_cases = golden_dataset[:5]
        results = []
        
        story_session_service = story_runner.session_service
        eval_session_service = evaluator_runner.session_service
        
        for i, test_case in enumerate(test_cases):
            test_id = test_case["id"]
            topic = test_case["initial_state"]["topic"]
            query = test_case["query"]
            
            print(f"\n{'='*60}")
            print(f"Running test case: {test_id}")
            print(f"Topic: {topic}")
            
            # Create sessions
            story_session_id = f"story_{test_id}"
            eval_session_id = f"eval_{test_id}"
            
            await story_session_service.create_session(
                app_name=APP_NAME,
                user_id=USER_ID,
                session_id=story_session_id,
                state={"topic": topic}
            )
            
            await eval_session_service.create_session(
                app_name=f"{APP_NAME}_evaluator",
                user_id=USER_ID,
                session_id=eval_session_id
            )
            
            # Generate story
            story = await run_story_agent(story_runner, story_session_id, topic, query)
            print(f"Generated story ({len(story)} chars)")
            
            # Evaluate story
            eval_input = EvaluationInput(
                test_id=test_id,
                topic=topic,
                generated_story=story if story else "No story generated",
                expected_behavior=test_case["expected_behavior"],
                evaluation_criteria=test_case["evaluation_criteria"]
            )
            
            eval_result = await run_evaluator(runner=evaluator_runner, session_id=eval_session_id, eval_input=eval_input)
            
            eval_result["generated_story_preview"] = story[:200] if story else "None"
            results.append(eval_result)
            
            print(f"Verdict: {eval_result.get('verdict', 'N/A')}")
            print(f"Score: {eval_result.get('overall_score', 'N/A')}")
        
        # Summary
        print(f"\n{'='*60}")
        print("EVALUATION SUMMARY")
        print(f"{'='*60}")
        
        pass_count = sum(1 for r in results if r.get("verdict") == "PASS")
        partial_count = sum(1 for r in results if r.get("verdict") == "PARTIAL")
        fail_count = sum(1 for r in results if r.get("verdict") == "FAIL")
        error_count = sum(1 for r in results if r.get("verdict") == "ERROR")
        
        print(f"Total Tests: {len(results)}")
        print(f"PASS: {pass_count}")
        print(f"PARTIAL: {partial_count}")
        print(f"FAIL: {fail_count}")
        print(f"ERROR: {error_count}")
        
        avg_score = sum(r.get("overall_score", 0) for r in results) / len(results) if results else 0
        print(f"Average Score: {avg_score:.2f}")
        
        # Save results to file
        results_path = Path(__file__).parent / "data" / "evaluation_results.json"
        with open(results_path, "w") as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "summary": {
                    "total": len(results),
                    "pass": pass_count,
                    "partial": partial_count,
                    "fail": fail_count,
                    "error": error_count,
                    "average_score": avg_score
                },
                "results": results
            }, f, indent=2)
        
        print(f"\nResults saved to: {results_path}")
        
        # Assert minimum quality
        assert avg_score >= 0.5, f"Average score {avg_score} is below minimum threshold 0.5"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
